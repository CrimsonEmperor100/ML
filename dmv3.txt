import pandas as pd
import numpy as np
import seaborn as sns

data = pd.read_csv('TelecomCustomer_Churn.csv')
data.head()


# 2. Explore the dataset to understand its structure and content
data.info()

data.shape

data.describe()

# 3.Remove any duplicate records from the dataset
data[data.duplicated(subset=['customerID'])].T

data = data.drop_duplicates(subset=['customerID'])

data.shape


# 4.Check for inconsistent data, such as inconsistent formatting or spelling variations, and standardize it
data['gender'].value_counts()

data['gender'] = data['gender'].replace({'F':'Female','M ':'Male'})

data.gender.value_counts()

# 5.Handle missing values in the dataset, deciding on an appropriate strategy
data.isnull().sum()

categorical_cols = ['gender','Partner','Dependents','PhoneService','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','Contract','StreamingMovies','PaymentMethod','Churn']
numercial_cols = ['MonthlyCharges','tenure','TotalCharges']

for col in categorical_cols:
    data[col].fillna(data[col].mode()[0],inplace=True)

for col in numercial_cols:
    data[col].fillna(data[col].mean(),inplace=True)

data.isna().sum()


# 6.Convert columns to the correct data types as needed.
data.dtypes

data[['tenure','MonthlyCharges','TotalCharges']] = data[['tenure','MonthlyCharges','TotalCharges']].astype('int64')
data.dtypes


# 7. Identify and handle outliers in the data.
# Detecting outliers
outliers = data[(data["MonthlyCharges"] > 3.5 * data["MonthlyCharges"].std()) | (data["MonthlyCharges"] < -3.5 * data["MonthlyCharges"].std())]
outliers
# Removed outliers
data = data[(data["MonthlyCharges"] < 3.5 * data["MonthlyCharges"].std()) & (data["MonthlyCharges"] > -3.5 * data["MonthlyCharges"].std())]

data.shape


# 8.Perform feature engineering, creating new features that may be relevant to predicting customer churn
data['early_churn'] = (data['tenure'] <= 12) & (data['Churn'] == 'Yes')

data.head()


# 9. Normalize or scale the data if necessary
from sklearn.preprocessing import Normalizer as n

data[['MonthlyCharges','TotalCharges']] = n().fit_transform(data[['MonthlyCharges','TotalCharges']])

data.head()


# 10.Split the dataset into training and testing sets for further analysis
from sklearn.model_selection import train_test_split

X = data.drop('Churn',axis=1)
y = data['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
X_train.shape

data.shape


# 11. Export the cleaned dataset for future analysis or modeling.
X_train.to_csv('Xtrain.csv')
X_test.to_csv('Xtest.csv')